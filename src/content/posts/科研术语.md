---
title: 科研术语
published: 2024-12-05T00:00:00.000Z
description: ''
image: ''
tags:
  - notes
category: 科研
draft: false
lang: ''
---
***Content***

<!-- toc -->

- [MoE(mixture of eXPERTS)](#moemixture-of-experts)
- [异构LLMs / 同构LLMs](#%E5%BC%82%E6%9E%84llms----%E5%90%8C%E6%9E%84llms)
- [Model Fusion（模型融合）](#model-fusion%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88)
- [diffusion 架构不兼容](#diffusion--%E6%9E%B6%E6%9E%84%E4%B8%8D%E5%85%BC%E5%AE%B9)
- [transformer](#transformer)
- [RNN](#rnn)
- [CNN](#cnn)
- [FFN (Feed-forward Network)](#ffn-feed-forward-network)
- [Soft Prompt](#soft-prompt)
- [Prediction Heads](#prediction-heads)
- [KL Divergence（Kullback-Leibler散度）](#kl-divergencekullback-leibler%E6%95%A3%E5%BA%A6)
- [Distance Functions in LLM (Large Language Models)](#distance-functions-in-llm-large-language-models)
- [Distance Functions in LLM (Large Language Models)](#distance-functions-in-llm-large-language-models-1)
- [Cross-Entropy Loss](#cross-entropy-loss)
- [Vocabulary in Large Models](#vocabulary-in-large-models)
- [Softmax 操作](#softmax-%E6%93%8D%E4%BD%9C)
- [Hidden States](#hidden-states)
- [Attention 注意力机制](#attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)
- [Transformer 模型架构中不同概念的关联](#transformer-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E4%B8%8D%E5%90%8C%E6%A6%82%E5%BF%B5%E7%9A%84%E5%85%B3%E8%81%94)
- [Transformer 模型架构与公式解析](#transformer-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90)
- [深度学习（Deep Learning）](#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0deep-learning)
- [Transformer 中的神经网络层构成与分类](#transformer-%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82%E6%9E%84%E6%88%90%E4%B8%8E%E5%88%86%E7%B1%BB)

<!-- tocstop -->

#  MoE(mixture of eXPERTS)
**MoE** 是 **Mixture of Experts** 的缩写，中文通常译为“专家混合模型”。这是一个机器学习和深度学习中的模型架构，特别适用于解决大规模和复杂任务。以下是 MoE 的主要概念和特点：

**核心思想**

MoE 的核心思想是使用多个“专家”模型（或子模型），并根据输入数据的特征，动态地选择和激活最适合的一个或多个专家进行处理。一个“门控机制”（Gate）会负责控制专家的选择，确保不同的输入由最合适的专家处理。

**MoE 的组成部分**

1. **专家网络（Experts）**
* 一组子网络，每个网络通常是一个深度神经网络。每个专家专注于特定的数据模式或任务
2. **门控网络（Gating Network）**
*  负责根据输入生成一个权重向量，表示对每个专家的信任程度。门控网络输出的权重决定了哪些专家会被激活以及各自的参与程度。
3. **组合机制**
*  根据门控网络的权重，将选中的专家的输出**加权求和**，生成最终输出。

**优点**
1. **参数规模扩展性**
*  MoE 使用**稀疏激活**的方式（只激活部分专家），在增加模型容量的同时有效控制计算成本。
2. **任务分解能力**
* 每个专家可以专注于学习特定类型的数据模式或任务，提升整体模型的表现力。
3. **计算效率**
* 虽然模型参数很多，但每次只计算一小部分专家，大幅降低了推理时间。

**应用场景**
1. **自然语言处理（NLP）**
* MoE 被广泛用于大规模预训练语言模型（如 Google 的 Switch Transformer 和 GLaM 模型），以提升训练效率和模型性能。
2. **计算机视觉（CV）**
* 在图像分类、目标检测等任务中，通过 MoE 来处理不同风格或类别的图像。
3. **多任务学习**
* MoE 可以通过不同专家处理不同任务，方便任务间共享信息。

**MoE 的挑战**
1. **负载均衡问题**
* 需要设计好的门控机制，确保专家被均匀使用，避免某些专家过载或长期闲置。
2. **训练稳定性**
* 动态选择机制可能导致训练不稳定或梯度更新不平衡。
3. **稀疏激活优化**
* 在稀疏激活时，需要高效实现专家的动态选择和调度。

**总结**
MoE 是一种高效的模型架构，特别适合参数规模巨大的深度学习任务。通过动态选择专家处理输入，MoE 实现了高性能与计算效率的平衡。




# 异构LLMs  /  同构LLMs

在构建大型语言模型（Large Language Models, LLMs）系统时，**异构 LLMs** 和 **同构 LLMs** 是两种不同的架构设计理念。它们的核心区别在于：参与任务的模型是否为相同类型及是否共享参数。

---

## 1. **同构 LLMs (Homogeneous LLMs)**

### 定义
- **同构 LLMs** 指由多个完全相同或高度相似的模型组成的系统，这些模型具有一致的架构、参数规模或任务能力。
- **特点**：
  1. **共享模型**：所有 LLMs 的架构相同，通常使用相同的参数。
  2. **一致性**：模型的行为表现具有一致性，例如生成风格或推理逻辑一致。

### 应用场景
- **负载均衡**：在高并发场景下，为提升响应速度，多个相同的 LLM 实例同时处理不同用户请求。
- **分布式计算**：使用多个相同模型协同计算大规模推理任务。
- **流水线优化**：模型实例在同一个任务中负责不同的输入批次，但逻辑完全相同。

### 优点
- **实现简单**：由于所有实例相同，管理和训练成本较低。
- **性能一致**：不同实例的推理结果在相同条件下是可预测的。

### 缺点
- **灵活性不足**：对于多样化任务场景，模型缺乏针对性优化。
- **资源利用率可能较低**：无法根据具体任务调整模型大小或功能。

---

## 2. **异构 LLMs (Heterogeneous LLMs)**

### 定义
- **异构 LLMs** 指由多个不同的模型组成的系统，这些模型可以具有不同的架构、参数规模、训练目标或专长领域。
- **特点**：
  1. **模型多样性**：系统中的 LLMs 可以是大模型、小模型、专用模型等的组合。
  2. **任务专注性**：每个模型在系统中承担特定任务，形成协同效应。

### 应用场景
- **多模态系统**：一个系统中可能包含文本生成模型、图像处理模型和语音识别模型。
- **任务分工**：不同模型处理不同类型的任务，例如一个模型负责自然语言生成，另一个负责逻辑推理。
- **边缘计算与云计算结合**：小模型用于本地推理，大模型用于云端深度计算。

### 优点
- **灵活性强**：可以针对不同任务定制模型，从而提高性能。
- **资源优化**：不同任务可以使用不同大小或架构的模型，避免资源浪费。
- **增强能力**：组合多个专用模型可以实现单个模型无法完成的复杂任务。

### 缺点
- **系统复杂性高**：需要设计任务分配、模型协调的机制，增加工程和管理成本。
- **开发成本较高**：不同模型可能需要独立训练和优化。

---

## 3. **异构 LLMs 和 同构 LLMs 的对比**

| **特性**             | **同构 LLMs**                      | **异构 LLMs**                      |
|----------------------|-----------------------------------|-----------------------------------|
| **模型架构**          | 相同模型架构或实例                 | 不同模型架构                     |
| **任务分工**          | 通常分工不明确或通用化             | 专门分工（如文本生成、逻辑推理等） |
| **灵活性**           | 较低                              | 较高                              |
| **复杂性**           | 较低                              | 较高                              |
| **计算资源利用**       | 通常较低                          | 较优                              |
| **典型应用**          | 高并发响应、通用任务处理           | 多模态任务、特定领域任务           |

---

## 4. **示例**

### 同构 LLMs
- 一个基于 GPT 的聊天系统同时运行 10 个 GPT-4 实例，用于处理多个用户的对话请求。所有实例模型一致，主要区别在于处理的用户输入不同。

### 异构 LLMs
- 一个多模态对话系统：
  1. GPT-4：负责自然语言生成。
  2. CLIP：负责图像与文本的相关性理解。
  3. Whisper：负责语音转文本任务。
  4. 专用逻辑推理模型：用于复杂推理任务。

---

## 总结
- **同构 LLMs** 适用于通用任务或高并发需求，强调一致性和高效性。
- **异构 LLMs** 强调灵活性和任务专注性，适合多样化或复杂任务。
- 系统的选择取决于应用场景的需求以及资源条件。

# Model Fusion（模型融合）

模型融合指在机器学习或深度学习中，将多个模型的预测结果进行组合，以提高整体性能或增强模型的鲁棒性。它可以利用不同模型的优势，减少单一模型可能出现的过拟合或偏差，从而获得更好的泛化能力。

---

## **模型融合的动机**

1. **减小误差**  
   - 通过综合多个模型的预测，可以减少单一模型可能存在的偏差或噪声。
2. **增强鲁棒性**  
   - 不同模型对数据分布的敏感性不同，融合多个模型可以增强对异常或未知样本的适应能力。
3. **利用多样性**  
   - 不同的模型（或同一模型的不同版本）可能在特定区域表现更优，融合可以充分利用这些特性。

---

## **模型融合的方法**

### 1. **简单融合**
- 直接对多个模型的预测结果进行加权或平均。

#### 方法：
- **平均融合（Averaging）**：对各模型的预测取平均值。
- **加权融合（Weighted Averaging）**：为不同模型分配不同权重，根据其性能调整权重大小。

---

### 2. **投票法（Voting）**
- 适用于分类任务，根据多个模型的预测结果进行多数表决。

#### 方法：
- **硬投票（Hard Voting）**：选择多数模型预测的类别作为最终结果。
- **软投票（Soft Voting）**：根据各模型的类别概率分布加权平均，选取概率最大的类别。

---

### 3. **堆叠（Stacking）**
- 使用另一个模型（元模型）对多个基础模型的预测结果进行学习，从而得到最终预测。

#### 过程：
1. 训练多个基础模型（如决策树、SVM、神经网络等）。
2. 使用基础模型的预测结果作为新特征，训练一个元模型（如线性回归或更复杂的模型）。
3. 元模型输出最终结果。

---

### 4. **Bagging（自助法，Bootstrap Aggregating）**
- 利用不同数据子集训练多个模型，并对其预测结果进行融合。
- **随机森林** 是典型的 Bagging 应用。

---

### 5. **Boosting**
- 按序列训练多个模型，每个模型关注前一模型未能正确处理的样本。
- 最终模型通过加权方式融合各阶段模型的结果。

#### 典型方法：
- Adaboost、Gradient Boosting、XGBoost、LightGBM。

---

### 6. **模型混合（Ensemble of Ensembles）**
- 多层次的融合方法，将上述方法结合使用，进一步提高性能。

---

## **模型融合的场景**

### 1. **分类任务**
- 在多个分类器之间进行投票或加权平均，减少单个分类器的误差。

### 2. **回归任务**
- 将多个回归模型的输出进行平均或堆叠，提升预测精度。

### 3. **深度学习中的模型融合**
- 在深度学习中，常用以下融合策略：
  - **多模型融合**：融合多个不同架构的深度学习模型（如 CNN 和 Transformer）。
  - **多任务学习**：通过共享特征提取层，将多任务模型的预测结果进行融合。
  - **多模态数据融合**：将来自不同数据源（如图像、文本、音频）的模型融合在一起。

---

## **模型融合的优缺点**

### **优点**
1. **性能提升**
   - 融合方法通常能提高预测精度，特别是在模型间具有一定多样性的情况下。
2. **鲁棒性增强**
   - 减少单个模型对特定样本或噪声的过度敏感。
3. **广泛适用**
   - 适用于分类、回归、生成任务，以及多模态数据。

### **缺点**
1. **计算开销大**
   - 需要训练多个模型，融合过程可能涉及额外的优化步骤。
2. **复杂性提高**
   - 模型融合引入了新的超参数（如权重分配），优化难度增加。
3. **依赖模型多样性**
   - 如果融合的模型过于相似，可能不会显著提升性能。

---

## **应用案例**

### 1. **比赛和排行榜**
- 在 Kaggle 等机器学习竞赛中，模型融合几乎是夺冠的必要手段。
- 通常采用 Stacking 或 Voting，结合多个强力模型（如 XGBoost 和深度学习模型）。

### 2. **生产环境**
- 在工业系统中，多个模型融合可以增强系统的鲁棒性，减少对单一模型失效的风险。

### 3. **深度学习中的应用**
- **图像任务**：在目标检测、图像分类任务中，融合多个 CNN 模型结果。
- **NLP 任务**：结合不同语言模型（如 BERT、GPT）的输出以提高文本生成或分类的性能。

---

## 总结
模型融合是一种通过集成多个模型提升整体性能的技术方法，具有广泛的应用价值。虽然融合过程增加了计算成本和复杂性，但在性能提升、鲁棒性增强等方面具有明显的优势。
# diffusion  架构不兼容
# transformer
# RNN
# CNN

# FFN (Feed-forward Network)
# Soft Prompt

# Prediction Heads

## 定义
在机器学习和深度学习中，**Prediction Heads**（预测头）是模型中的组件，通常用来将前面提取到的特征（如通过神经网络提取的特征表示）映射到最终的输出空间。  
它们常见于多任务学习、目标检测、自然语言处理等领域，主要功能包括：
1. **转换特征到输出**：将网络前面提取到的高维特征映射到目标任务的输出（如分类分数、边界框坐标、语言模型的下一个词预测等）。
2. **任务专属功能**：在多任务学习中，预测头专门针对某一任务（如分类、回归、分割等）做进一步处理。

---

## 组成
预测头的组成可能因任务而异，但通常包括以下部分：
- **全连接层**（Fully Connected Layers）：实现特征到输出的映射。
- **激活函数**（如softmax、sigmoid）：根据任务需求（分类或回归）进行非线性变换。
- **损失计算模块**：有时损失函数也是预测头的一部分，用于指导模型训练。

---

## 应用场景
### 1. **多任务学习**
在多任务学习中，模型共享一个主干（backbone）网络提取特征，但使用多个预测头为每个任务提供单独的输出。例如：
- 在图像处理任务中，可能有一个预测头用于分类，另一个用于边界框回归。

### 2. **目标检测**
像 **Faster R-CNN** 或 **YOLO** 这样的目标检测模型中，通常包含两个预测头：
- **分类头**：输出类别概率。
- **回归头**：输出边界框的坐标。

### 3. **自然语言处理**
在语言模型（如BERT、GPT）中：
- **词预测头**：预测下一个词或掩盖的词。
- **分类头**：在特定任务（如情感分类）中，输出对应类别的概率。

### 4. **Transformer模型**
Transformer模型中的预测头通常用于：
- 将Transformer的编码器输出映射到特定任务的目标空间。
- 用于生成或分类等任务。

---

## 示例
以BERT为例：
- 主体模型提取文本特征。
- 一个 **CLS预测头** 将 `CLS` token 的特征向量映射到分类任务的类别概率。

以下是使用PyTorch实现的简单预测头代码：
```python
import torch.nn as nn

class PredictionHead(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(PredictionHead, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim)
        self.activation = nn.Softmax(dim=-1)  # 针对分类任务

    def forward(self, x):
        x = self.fc(x)
        x = self.activation(x)
        return x

```


# KL Divergence（Kullback-Leibler散度）

## 定义
KL散度（Kullback-Leibler Divergence）是一种用于衡量两个概率分布之间差异的非对称度量。  
它可以理解为：**从分布P到分布Q所需的额外信息量**，或者是**分布P与分布Q之间的相对熵**。

公式如下：
$$
D_{KL}(P || Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
$$
对于连续分布：
$$
D_{KL}(P || Q) = \int P(x) \log\frac{P(x)}{Q(x)} dx
$$

其中：
- $P(x)$：真实分布（通常是目标分布）。
- $Q(x)$：近似分布（通常是模型预测分布）。
- $\log$：以自然对数为底。

---

## 性质
1. **非负性**：  
   $$
   D_{KL}(P || Q) \geq 0
   $$
   当且仅当 \(P(x) = Q(x)\) 对所有 \(x\) 成立时，\(D_{KL}(P || Q) = 0\)。

2. **非对称性**：  
   $$
   D_{KL}(P || Q) \neq D_{KL}(Q || P)
   $$  
   因此，KL散度不是一个严格意义上的距离度量。

3. **对概率分布敏感**：  
   如果 \(Q(x)\) 在某些点的概率非常低甚至为零，而 \(P(x)\) 较大，则 KL 散度会趋向无穷大。

---

## 直观解释
1. **信息增益**：KL散度衡量的是用 $Q(x)$ 近似 $P(x)$时的额外编码开销。  
   - 如果 $Q$ 和 $P$ 的分布非常相近，那么 KL 散度会很小。
   - 如果 $Q$ 和 $P$ 差异很大，那么 KL 散度会很大。

2. **模型训练**：在机器学习中，KL散度常用于评估模型预测分布 $Q(x)$ 与目标分布 $P(x)$ 的接近程度。例如：
   - 在分类任务中，用KL散度最小化模型预测分布和真实分布之间的差异。

---

## 应用场景
1. **概率分布匹配**：
   - 用于优化生成模型（如变分自编码器，VAE）。
   - 衡量预测分布和目标分布的差异。

2. **自然语言处理**：
   - 比较语言模型生成的概率分布和真实文本分布。

3. **信息论**：
   - 量化信息增益或相对熵。

4. **强化学习**：
   - 用于限制策略更新时的分布变化（如TRPO算法）。

---

## 示例代码
使用Python计算KL散度：
```python
import numpy as np
from scipy.stats import entropy

# 两个分布
P = np.array([0.4, 0.6])  # 真实分布
Q = np.array([0.5, 0.5])  # 近似分布

# KL散度
kl_divergence = entropy(P, Q)
print(f"KL散度: {kl_divergence}")
```

# Distance Functions in LLM (Large Language Models)

## 定义
在大语言模型（LLM）中，**Distance Functions** 是用来度量两个向量、概率分布或嵌入之间差异或相似性的数学工具。  
这些函数在模型训练、推理、比较文本表示或优化目标中扮演着重要角色。

---

## 常见的距离函数

### 1. **欧几里得距离（Euclidean Distance）**
计算两个向量之间的直线距离，公式为：
$$
d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
$$
- **用途**：用于衡量向量的绝对差异。
- **限制**：对尺度敏感。

---

### 2. **余弦相似度（Cosine Similarity）**
衡量两个向量方向的相似性，定义为：
$$
\text{Cosine Similarity}(x, y) = \frac{x \cdot y}{\|x\| \|y\|}
$$
- **范围**：$[-1, 1]$。
- **转换为距离**：
$$
d(x, y) = 1 - \text{Cosine Similarity}(x, y)
$$
- **用途**：用于文本嵌入表示的相似性比较，尤其当向量的模不重要时。

---

### 3. **曼哈顿距离（Manhattan Distance）**
计算两个向量之间的绝对差值之和：
$$
d(x, y) = \sum_{i=1}^{n} |x_i - y_i|
$$
- **用途**：在高维空间中，适合稀疏向量的距离计算。

---

### 4. **KL散度（Kullback-Leibler Divergence）**
用于衡量两个概率分布之间的差异：
$$
D_{KL}(P || Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
$$
- **用途**：衡量模型预测分布和目标分布之间的差异。
- **限制**：不对称，即 $D_{KL}(P || Q) \neq D_{KL}(Q || P)$。

---

### 5. **Jensen-Shannon散度（Jensen-Shannon Divergence）**
KL散度的对称化版本，定义为：
$$
D_{JS}(P || Q) = \frac{1}{2} D_{KL}(P || M) + \frac{1}{2} D_{KL}(Q || M)
$$
其中 $M = \frac{P + Q}{2}$。
- **范围**：$[0, 1]$。
- **用途**：用于对称分布比较。

---

### 6. **Hamming距离（Hamming Distance）**
衡量两个向量（或字符串）之间不同位置的数量：
$$
d(x, y) = \sum_{i=1}^{n} \mathbf{1}(x_i \neq y_i)
$$
- **用途**：主要用于离散数据（如二进制向量）的比较。

---

### 7. **Wasserstein距离（Earth Mover's Distance）**
用于衡量两个分布之间的最小“搬运成本”：
$$
W(P, Q) = \inf_{\gamma \in \Gamma(P, Q)} \int |x - y| d\gamma(x, y)
$$
- **用途**：在生成模型（如GAN）中，用于比较生成分布与真实分布。

---

## LLM中的应用
1. **嵌入比较**：用余弦相似度或欧几里得距离比较文本或词向量的相似性。
2. **分布差异**：用KL散度或Jensen-Shannon散度评估模型输出概率分布与目标分布的接近程度。
3. **模型优化**：Wasserstein距离用于优化生成模型（如 Wasserstein GAN）。

---

## 示例代码
以下是计算余弦相似度和KL散度的Python代码：
```python
import numpy as np
from scipy.spatial.distance import cosine
from scipy.stats import entropy

# 示例向量
x = np.array([1, 2, 3])
y = np.array([2, 4, 6])

# 余弦相似度
cos_sim = 1 - cosine(x, y)
print(f"余弦相似度: {cos_sim}")

# 概率分布
P = np.array([0.4, 0.6])
Q = np.array([0.5, 0.5])

# KL散度
kl_div = entropy(P, Q)
print(f"KL散度: {kl_div}")
```

# Distance Functions in LLM (Large Language Models)

## 定义
在大语言模型（LLM）中，**Distance Functions** 是用来度量两个向量、概率分布或嵌入之间差异或相似性的数学工具。  
这些函数在模型训练、推理、比较文本表示或优化目标中扮演着重要角色。

---

## 常见的距离函数

### 1. **欧几里得距离（Euclidean Distance）**
计算两个向量之间的直线距离，公式为：
$$
d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
$$
- **用途**：用于衡量向量的绝对差异。
- **限制**：对尺度敏感。

---

### 2. **余弦相似度（Cosine Similarity）**
衡量两个向量方向的相似性，定义为：
$$
\text{Cosine Similarity}(x, y) = \frac{x \cdot y}{\|x\| \|y\|}
$$
- **范围**：$[-1, 1]$。
- **转换为距离**：
$$
d(x, y) = 1 - \text{Cosine Similarity}(x, y)
$$
- **用途**：用于文本嵌入表示的相似性比较，尤其当向量的模不重要时。

---

### 3. **曼哈顿距离（Manhattan Distance）**
计算两个向量之间的绝对差值之和：
$$
d(x, y) = \sum_{i=1}^{n} |x_i - y_i|
$$
- **用途**：在高维空间中，适合稀疏向量的距离计算。

---

### 4. **KL散度（Kullback-Leibler Divergence）**
用于衡量两个概率分布之间的差异：
$$
D_{KL}(P || Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
$$
- **用途**：衡量模型预测分布和目标分布之间的差异。
- **限制**：不对称，即 $D_{KL}(P || Q) \neq D_{KL}(Q || P)$。

---

### 5. **Jensen-Shannon散度（Jensen-Shannon Divergence）**
KL散度的对称化版本，定义为：
$$
D_{JS}(P || Q) = \frac{1}{2} D_{KL}(P || M) + \frac{1}{2} D_{KL}(Q || M)
$$
其中 $M = \frac{P + Q}{2}$。
- **范围**：$[0, 1]$。
- **用途**：用于对称分布比较。

---

### 6. **Hamming距离（Hamming Distance）**
衡量两个向量（或字符串）之间不同位置的数量：
$$
d(x, y) = \sum_{i=1}^{n} \mathbf{1}(x_i \neq y_i)
$$
- **用途**：主要用于离散数据（如二进制向量）的比较。

---

### 7. **Wasserstein距离（Earth Mover's Distance）**
用于衡量两个分布之间的最小“搬运成本”：
$$
W(P, Q) = \inf_{\gamma \in \Gamma(P, Q)} \int |x - y| d\gamma(x, y)
$$
- **用途**：在生成模型（如GAN）中，用于比较生成分布与真实分布。

---

## LLM中的应用
1. **嵌入比较**：用余弦相似度或欧几里得距离比较文本或词向量的相似性。
2. **分布差异**：用KL散度或Jensen-Shannon散度评估模型输出概率分布与目标分布的接近程度。
3. **模型优化**：Wasserstein距离用于优化生成模型（如 Wasserstein GAN）。

---

## 示例代码
以下是计算余弦相似度和KL散度的Python代码：
```python
import numpy as np
from scipy.spatial.distance import cosine
from scipy.stats import entropy

# 示例向量
x = np.array([1, 2, 3])
y = np.array([2, 4, 6])

# 余弦相似度
cos_sim = 1 - cosine(x, y)
print(f"余弦相似度: {cos_sim}")

# 概率分布
P = np.array([0.4, 0.6])
Q = np.array([0.5, 0.5])

# KL散度
kl_div = entropy(P, Q)
print(f"KL散度: {kl_div}")
```

# Cross-Entropy Loss

## 定义
**Cross-Entropy Loss**（交叉熵损失）是一种常用的损失函数，特别适用于分类任务。它衡量预测分布与真实分布之间的差异。  
在模型训练中，交叉熵损失能够更好地引导模型输出接近目标概率分布。

对于一个样本，其交叉熵损失定义为：
$$
L = -\sum_{i=1}^C y_i \log(\hat{y}_i)
$$
其中：
- $C$：类别总数。
- $y_i$：真实类别的独热编码（one-hot encoded）值，$y_i \in \{0, 1\}$。
- $\hat{y}_i$：模型预测的概率分布，通常是通过 softmax 输出的。

---

## 简化公式
对于单个样本，假设真实类别为 $j$，交叉熵损失可以简化为：
$$
L = -\log(\hat{y}_j)
$$
即仅取目标类别的预测概率的对数。

---

## Softmax 和 Cross-Entropy 的关系
交叉熵损失通常与 softmax 函数结合使用：
1. **Softmax** 用于将模型的原始输出（logits）转换为概率分布：
   $$
   \hat{y}_i = \frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)}
   $$
   其中 $z_i$ 是模型对类别 $i$ 的原始预测值。

2. **Cross-Entropy Loss** 对 softmax 输出的概率分布和目标分布进行比较。

---

## 应用场景
1. **分类任务**：
   - 用于多分类问题（如图像分类、文本分类等）。
   - 二分类问题是交叉熵损失的特例。

2. **概率分布匹配**：
   - 衡量模型预测概率分布与真实分布的差异。

---

## 特点
1. **目标导向**：交叉熵惩罚错误预测更大的类别，鼓励模型输出更接近目标类别的概率分布。
2. **非负性**：交叉熵损失始终为非负值，当预测完全正确时损失为 0。
3. **对数缩放**：通过取对数，降低了预测置信度过低时的影响。

---

## 二分类问题
对于二分类问题（$C=2$），真实标签 $y \in \{0, 1\}$，交叉熵损失公式可以简化为：
$$
L = -y \log(\hat{y}) - (1-y) \log(1-\hat{y})
$$
其中 $\hat{y}$ 是正类的预测概率。

---

## 示例代码
以下是用 Python 计算交叉熵损失的代码：

```python
import numpy as np

# 真实标签（独热编码）
y = np.array([0, 1, 0])  # 类别 2 为真实类别

# 模型预测的概率分布
y_hat = np.array([0.1, 0.7, 0.2])

# 交叉熵损失
loss = -np.sum(y * np.log(y_hat))
print(f"交叉熵损失: {loss}")
```

# Vocabulary in Large Models

## 定义
在大语言模型（LLM）中，**Vocabulary** 是指模型能够处理的所有唯一词或子词（tokens）的集合。  
模型通过将输入文本分解为这些 tokens，并将其映射为数值表示来处理语言任务。  

---

## Vocabulary 的组成
1. **词级别（Word-level Vocabulary）**：
   - 每个单词作为一个 token，例如 "apple" 和 "orange" 是两个不同的词。
   - 问题：需要非常大的词表来覆盖所有可能的单词，容易导致稀疏性。

2. **子词级别（Subword-level Vocabulary）**：
   - 将单词拆分为子词（subwords），例如 "unbelievable" 可能分为 "un", "believ", 和 "able"。
   - 常用算法：
     - **BPE**（Byte Pair Encoding）：基于频率合并子词。
     - **WordPiece**：常用于 BERT 等模型，类似于 BPE。
     - **SentencePiece**：适用于任何语言，不依赖空格分割。

3. **字符级别（Character-level Vocabulary）**：
   - 每个字符作为一个 token，例如 "cat" 被分解为 ["c", "a", "t"]。
   - 问题：虽然能够处理所有语言，但生成较长序列时效率较低。

4. **字节级别（Byte-level Vocabulary）**：
   - 每个字节（byte）作为 token，例如 GPT-2 使用的 Byte Pair Encoding 能很好地处理跨语言文本。

---

## Vocabulary Size 的规定
**Vocabulary Size** 是指词表中 token 的数量。  
- **大小选择的权衡**：
  - 过大：增加模型复杂度，占用更多内存，训练时间变长。
  - 过小：会导致更多的分词，增加序列长度，降低处理效率。

### 常见模型的 Vocabulary Size
| 模型       | Vocabulary Size | 描述                                |
|------------|----------------|-------------------------------------|
| GPT-2      | 50,257         | 使用 Byte-Level BPE。               |
| BERT       | 30,000         | 使用 WordPiece。                    |
| GPT-3      | ~50,000        | 适配多语言，字节级 BPE。            |
| RoBERTa    | 50,265         | 基于 BERT 的改进，扩展词表。         |

### 决定 Vocabulary Size 的因素
1. **语言特性**：
   - 单词多变的语言（如英语）需要更大的词表。
   - 表意语言（如中文）则需考虑字符或字节级处理。

2. **任务需求**：
   - 特殊任务（如领域特定词汇）可能需要额外扩展词表。

3. **模型大小**：
   - 更大的模型可以容纳更大的词表，但对小模型来说，过大的词表会导致稀疏性。

4. **硬件限制**：
   - 词表越大，需要的内存和计算资源越多。

---

## Vocabulary Size 的优化方法
1. **使用子词分解**：
   - 通过子词（BPE、WordPiece 等）减少词表大小，同时保持较高的语言覆盖率。
   
2. **去除低频词**：
   - 对低频词进行分解，减少无意义的 token。

3. **领域词汇扩展**：
   - 为领域专属模型添加特定术语。

4. **动态词表（Dynamic Vocabulary）**：
   - 在特定任务中生成适配的词表，而非使用固定词表。

---

## 示例：GPT-2 的 Byte-Level BPE
1. 输入文本被分割为字节流。
2. 常见的字节组合被逐步合并为 tokens。
3. 最终形成的词表大小为 50,257，覆盖多语言文本。

---

# Softmax 操作

## 定义
**Softmax** 是一种数学函数，常用于将一组任意实数转换为概率分布。  
给定一个向量 $z=[z_1,z_2,...,z_n]$，Softmax 的输出是一个长度为 $n$ 的概率向量，定义为：
$$
\text{Softmax}(z_i)=\frac{\exp(z_i)}{\sum_{j=1}^n\exp(z_j)}
$$
其中：
- $\exp(z_i)$：指数函数，确保所有值为正数。
- $\sum_{j=1}^n\exp(z_j)$：归一化项，使输出满足概率分布性质，即总和为1。

---

## 特性
1. **非负性**：Softmax 输出的每一项均为正数。
   $$
   \text{Softmax}(z_i)>0\quad\forall i
   $$

2. **归一化**：Softmax 输出的所有值加起来为1。
   $$
   \sum_{i=1}^n\text{Softmax}(z_i)=1
   $$

3. **指数放大**：Softmax 会放大数值差异较大的项，使较大的值占主导地位。

---

## 使用场景
1. **分类任务**：
   - 在多分类问题中，Softmax 用于将模型输出的logits转换为每个类别的概率。
   - 结合交叉熵损失（Cross-Entropy Loss）共同使用。

2. **概率建模**：
   - 用于将分数映射为概率分布，例如语言模型中下一个词的预测。

3. **注意力机制**：
   - 用于归一化注意力权重，使其表示概率分布。

---

## 示例
假设模型的原始输出（logits）为 $z=[2.0,1.0,0.1]$，计算 Softmax 输出：

1. 计算指数值：
   $$
   \exp(z)=[\exp(2.0),\exp(1.0),\exp(0.1)]\approx[7.39,2.72,1.11]
   $$

2. 计算总和：
   $$
   \text{sum}=7.39+2.72+1.11\approx11.22
   $$

3. 归一化：
   $$
   \text{Softmax}(z)=\left[\frac{7.39}{11.22},\frac{2.72}{11.22},\frac{1.11}{11.22}\right]\approx[0.66,0.24,0.10]
   $$

---

## Python 实现
以下是使用NumPy实现Softmax的代码：

```python
import numpy as np

def softmax(z):
    expz=np.exp(z-np.max(z)) # 防止数值溢出
    return expz/np.sum(expz)

# 示例
logits=np.array([2.0,1.0,0.1])
probs=softmax(logits)
print(probs) # 输出: [0.65900114 0.24243297 0.09856589]
```

# Hidden States

## 定义
在深度学习模型（尤其是语言模型）中，**hidden states** 指的是模型在每一层生成的中间特征表示。这些特征是输入通过模型后，在每一层网络中所提取的隐藏状态（hidden states）。  

它们反映了模型如何逐步将原始输入转化为更高层次的抽象特征，并在不同层次捕获语义信息。

---

## 特性
1. **逐层表示**：
   - 每一层的 hidden states 是该层网络对输入的特征提取结果，依赖于前一层的输出。
   - 例如，对于一个有 $L$ 层的 Transformer 模型，hidden states 通常表示为：
     $$
     \text{hidden states} = [h^{(0)}, h^{(1)}, ..., h^{(L)}]
     $$
     - $h^{(0)}$：表示输入的初始嵌入（embedding）。
     - $h^{(i)}$：表示第 $i$ 层的隐藏状态。

2. **维度**：
   - 对于输入序列长度为 $T$，隐藏层大小为 $H$，每一层的 hidden states 的维度为：
     $$
     h^{(i)} \in \mathbb{R}^{T \times H}
     $$
   - $T$ 是序列中 token 的数量，$H$ 是每个 token 的表示向量维度。

3. **最终输出层**：
   - 最后一层的 hidden states 通常被用于生成模型的预测结果（如分类概率、生成文本等）。

---

## 应用场景
1. **特征提取**：
   - Hidden states 可以作为中间表示，用于下游任务（如分类、序列标注、情感分析）。

2. **注意力分析**：
   - 可通过可视化 hidden states，理解模型在不同层如何捕获输入的语义信息。

3. **多层组合**：
   - 在某些场景下，结合多层 hidden states（而非仅使用最后一层）能提升下游任务的性能。

4. **解释性研究**：
   - 分析不同层 hidden states 的变化，研究模型学习到的模式。

---

## 示例
以下是基于 Hugging Face 的 Transformers 库，提取 BERT 模型的 hidden states 的示例代码：

```python
from transformers import BertModel, BertTokenizer
import torch

# 加载模型和分词器
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)

# 输入文本
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")

# 前向传播
with torch.no_grad():
    outputs = model(**inputs)

# 获取 hidden states
hidden_states = outputs.hidden_states  # 一个包含所有层 hidden states 的元组
print(f"Number of layers: {len(hidden_states)}")  # 通常包含模型层数 + 1 (embedding层)
print(f"Shape of hidden state for one layer: {hidden_states[-1].shape}")  # (batch_size, seq_len, hidden_size)
```

# Attention 注意力机制

## 定义
**注意力机制（Attention Mechanism）** 是一种在深度学习模型中模拟人类注意力的机制，能够动态地为输入序列中的不同部分分配权重。它帮助模型关注输入中最重要的信息，同时忽略不相关部分。

Attention 在序列建模任务（如翻译、摘要生成）中尤为重要，能够缓解传统 RNN 和 CNN 对长距离依赖的处理问题。

---

## 公式表示
### 输入与输出
假设输入是一个序列 $X = [x_1, x_2, ..., x_T]$，Attention 计算的是：
1. 每个输入 $x_i$ 与所有其他输入的关系（相关性分数）。
2. 使用这些分数加权输入生成输出表示。

### Scaled Dot-Product Attention
Attention 的计算公式如下：
1. **相关性分数（Score）**：
   $$
   \text{score}(q, k) = q \cdot k^T
   $$
   - $q$：查询向量（Query）。
   - $k$：键向量（Key）。

2. **归一化（Softmax）**：
   $$
   \alpha = \text{softmax}\left(\frac{\text{score}(q, k)}{\sqrt{d_k}}\right)
   $$
   - $\alpha$ 是注意力权重，表示每个输入对当前查询的贡献。
   - $\sqrt{d_k}$ 是缩放因子，用于防止数值过大。

3. **加权求和**：
   $$
   \text{Attention}(Q, K, V) = \alpha \cdot V
   $$
   - $V$：值向量（Value）。
   - 最终的 Attention 输出是值向量的加权和。

---

## 分类
### 1. **自注意力（Self-Attention）**
在自注意力中，查询、键和值都来源于同一序列。  
- 应用：Transformer 的核心机制。

### 2. **交叉注意力（Cross-Attention）**
查询、键和值分别来自不同的序列。
- 应用：机器翻译（源语言和目标语言间的注意力）。

### 3. **多头注意力（Multi-Head Attention）**
将注意力分为多个头，分别学习不同的子空间特征：
$$
\text{MultiHead}(Q, K, V) = [\text{head}_1, \text{head}_2, ..., \text{head}_h] \cdot W^O
$$
- 每个头单独计算 Attention。
- 最终将各头的结果拼接并线性变换。

---

## Transformer 中的应用
Transformer 通过多头自注意力完全取代了传统 RNN 的循环结构，实现了高效的全局信息建模。

### 示例：Transformer 中自注意力的计算流程
1. 输入序列 $X$ 通过线性变换生成 $Q, K, V$：
   $$
   Q = XW^Q, \quad K = XW^K, \quad V = XW^V
   $$
2. 计算 Attention：
   $$
   \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
   $$
3. 多头 Attention 并行执行，结果拼接后通过线性层输出。

---

## 示例代码
以下是 Scaled Dot-Product Attention 的实现：

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(query, key, value, mask=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)  # 掩码填充
    weights = F.softmax(scores, dim=-1)
    return torch.matmul(weights, value)

# 示例输入
q = torch.rand(2, 5, 64)  # (batch_size, seq_len, d_k)
k = torch.rand(2, 5, 64)
v = torch.rand(2, 5, 64)

output = scaled_dot_product_attention(q, k, v)
print(output.shape)  # 输出形状: (2, 5, 64)
```

# Transformer 模型架构中不同概念的关联

## 1. **Distribution of Probabilities** （概率分布）
- **关联部分**：**输出层（Output Layer）**
  - 概念描述：
    - 概率分布通常出现在 Transformer 模型的最后一层，用于对模型的输出进行归一化（如通过 Softmax 操作）。
    - 表示模型对每个可能类别的预测概率。
  - 具体实现：
    - Transformer 的输出经过线性层后，应用 **Softmax**，生成分类或下一个 token 的概率分布：
      $$
      P(y_i | x) = \text{softmax}(z_i) = \frac{\exp(z_i)}{\sum_{j=1}^N \exp(z_j)}
      $$
    - $z_i$ 是线性变换的输出，$N$ 是类别数量或词汇表大小。

---

## 2. **Representation Space** （表示空间）
- **关联部分**：**每一层的输出特征**（特别是最后一层隐藏状态）
  - 概念描述：
    - 表示空间是 Transformer 模型中通过编码器或解码器层提取的高维向量空间。
    - 不同 token 或句子在这个空间中被表示为向量，其分布可以反映语义相似性。
  - 具体实现：
    - **词级表示**：Transformer 层的每个 token 输出的向量都属于表示空间。
    - **句级表示**：对词级表示进行池化（如平均或最大池化）得到句子级别的表示。

---

## 3. **Hidden States** （隐藏状态）
- **关联部分**：**每一层的中间输出**
  - 概念描述：
    - Hidden States 是 Transformer 中间层的输出，表示输入数据在该层的特征表示。
    - 包括输入嵌入（embedding）和每一层经过自注意力和前馈神经网络的处理结果。
  - 具体实现：
    - 对于一个具有 $L$ 层的 Transformer 模型，hidden states 通常表示为：
      $$
      H = [h^{(0)}, h^{(1)}, ..., h^{(L)}]
      $$
      - $h^{(0)}$ 是输入的嵌入表示。
      - $h^{(i)}$ 是第 $i$ 层的输出。

---

## 总结表格

| **概念**               | **关联部分**             | **作用**                                                                                           |
|------------------------|-------------------------|--------------------------------------------------------------------------------------------------|
| Distribution of Probabilities | 输出层（Softmax 归一化）   | 提供对每个类别（或下一个 token）的预测概率，用于分类或生成。                                                |
| Representation Space   | 每一层（特别是最后一层的输出） | 表示输入数据的特征向量空间，用于捕获词语或句子的语义特征。                                                         |
| Hidden States          | 所有层的中间输出         | 逐层提取的特征表示，包含从低级特征到高级语义信息，用于多任务或多层级特征分析。                                              |


# Transformer 模型架构与公式解析
> 私链🔗：
> - [HPC101浙江大学超算短学期实验](https://zjusct.pages.zjusct.io/summer_hpc101_2024/hpc-101-labs-2024/Lab5-DL/)
> - [《Attention is All You Need》](https://arxiv.org/pdf/1706.03762)

Transformer 模型是由 Vaswani 等人于 2017 年提出的，它通过完全基于注意力机制（Attention Mechanism）来处理序列数据，尤其在自然语言处理（NLP）任务中取得了巨大的成功。与传统的 RNN 和 LSTM 模型不同，Transformer 使用自注意力（Self-Attention）和位置编码（Positional Encoding）来建模序列的长期依赖。

---

## Transformer 模型架构
![原理图](/media/sci/transformer.png)
Transformer 的架构由两个主要部分组成：
1. **编码器（Encoder）**：将输入序列映射到隐藏表示。
2. **解码器（Decoder）**：生成目标序列，通常用于序列到序列的任务（如机器翻译）。

### 编码器（Encoder）
- 编码器由 $N$ 层堆叠的相同模块组成，每个模块包括两个子层：
  1. **多头自注意力（Multi-Head Self-Attention）**
  2. **前馈神经网络（Feed Forward Neural Network）**

#### 1. **多头自注意力（Multi-Head Self-Attention）**
- **目的**：自注意力机制允许模型在处理当前单词时关注到输入序列中的其他所有单词，从而捕捉全局信息。
- **公式**：
  1. **查询、键、值（Query, Key, Value）**：
     $$ Q = XW^Q, \quad K = XW^K, \quad V = XW^V $$
     其中 $X$ 是输入（通常是词嵌入或前一层的输出），$W^Q$, $W^K$, $W^V$ 是权重矩阵。
  
  2. **计算注意力分数**（Scaled Dot-Product Attention）：
     $$ \text{score}(Q, K) = \frac{QK^T}{\sqrt{d_k}} $$ 
     - 这里，$d_k$ 是查询和键的维度，用于缩放避免过大的数值。
  
  3. **应用 Softmax 和加权求和**：
     $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

  4. **多头注意力**：
     $$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h)W^O $$
     - 每个头的计算独立进行，然后拼接并通过线性变换 $W^O$。

#### 2. **前馈神经网络（Feed Forward Neural Network）**
- **作用**：每个位置的输出都通过一个两层的前馈神经网络，通常包括 ReLU 激活函数。
- **公式**：
  $$ \text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2 $$

---

### 解码器（Decoder）
解码器的结构与编码器类似，但它有三个子层：
1. **多头自注意力（Masked Self-Attention）**：解码器只关注当前位置及之前的输出（masked）。
2. **编码-解码注意力（Encoder-Decoder Attention）**：这一步通过编码器的输出和解码器的输入计算注意力，帮助生成与输入相关的输出。
3. **前馈神经网络（Feed Forward Neural Network）**：与编码器相同。

#### 解码器中注意力的变更
- **Masked Self-Attention**：
  在解码器中，为了确保生成的 token 只能依赖于之前的 token（而不是未来的 token），需要对 self-attention 进行掩码处理：
  $$ \text{score}(Q, K) = \frac{QK^T}{\sqrt{d_k}} \quad \text{(mask future positions)} $$

- **Encoder-Decoder Attention**：
  在解码器的第二个子层，查询来自解码器输入，键和值来自编码器输出。通过这种方式，解码器能够关注编码器提取的输入信息。

---

## 位置编码（Positional Encoding）
由于 Transformer 完全依赖于注意力机制而没有递归结构，因此需要额外的 **位置编码** 来为模型提供序列中词语的顺序信息。位置编码是与输入嵌入相加的：
$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$
- 其中，$pos$ 是词语的位置信息，$i$ 是维度索引，$d$ 是嵌入维度。

---

## Transformer 模型的完整流程

1. **输入嵌入（Input Embedding）**：
   输入序列被映射到一个稠密的向量空间，并与位置编码相加。

2. **编码器**：
   编码器的每一层通过多头自注意力和前馈神经网络处理输入序列，生成序列的高维表示。

3. **解码器**：
   解码器使用编码器的输出和目标序列的部分信息生成目标序列的下一部分。

4. **输出层**：
   最后的解码器输出经过线性变换和 softmax 操作，生成目标序列中每个词汇的概率分布。

---

## Transformer 模型的优缺点

### 优点
1. **并行化**：Transformer 模型不像 RNN，能够并行处理整个输入序列，显著提高了训练效率。
2. **长距离依赖**：通过自注意力机制，Transformer 可以捕捉序列中的长距离依赖关系。

### 缺点
1. **计算复杂度**：自注意力机制的时间复杂度为 $O(T^2)$，在处理非常长的序列时计算开销较大。
2. **内存消耗**：需要存储大量的中间结果，尤其是在多头注意力机制和大规模训练时。

---

## 总结
Transformer 模型通过自注意力和前馈神经网络结构有效地处理了序列数据，尤其在 NLP 任务中得到了广泛应用。模型的关键在于注意力机制，它使得每个输入位置可以动态地关注到序列中其他位置的信息。通过堆叠多个编码器和解码器层，Transformer 实现了强大的建模能力。

# 深度学习（Deep Learning）

## 定义
深度学习是机器学习的一个分支，其核心思想是通过**多层神经网络**来模拟人脑的学习过程，从数据中自动提取特征并进行决策或生成。它是一种以**人工神经网络（Artificial Neural Networks, ANN）**为基础的技术，通常包含多个隐藏层（即“深度”）。

---

## 核心概念

### 1. **神经网络（Neural Networks）**
- 深度学习模型由神经网络构成，每个网络由**输入层**、**隐藏层**和**输出层**组成。
- 神经网络的基本单元是“神经元”，其计算如下：
  $$
  y = f\left(\sum_{i} w_i x_i + b\right)
  $$
  - $x_i$：输入特征。
  - $w_i$：权重。
  - $b$：偏置。
  - $f$：激活函数，用于引入非线性。

### 2. **深度（Depth）**
- 深度指的是神经网络中的层数，**深度学习模型**通常包含多层隐藏层，相比于传统的浅层模型，能够提取更复杂的特征。

### 3. **特征自动提取**
- 深度学习不需要手工设计特征，而是通过模型的训练自动学习特征表示。

### 4. **监督与非监督学习**
- **监督学习**：使用标注数据进行训练，例如图像分类、语音识别。
- **非监督学习**：使用无标签数据进行模式发现，例如聚类、生成模型。

---

## 工作流程

1. **数据准备**：
   - 收集并预处理数据（例如归一化、去噪）。
   - 通常需要大量的数据以避免过拟合。

2. **模型设计**：
   - 选择适当的神经网络架构（如卷积神经网络、循环神经网络、Transformer 等）。
   - 定义输入和输出的维度。

3. **模型训练**：
   - 利用反向传播算法（Backpropagation）和梯度下降优化权重。
   - 通过损失函数（如交叉熵、均方误差）评估模型性能：
     $$
     \mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \ell(\hat{y}_i, y_i)
     $$

4. **模型评估**：
   - 使用验证集检查模型的泛化性能。
   - 通过指标（如准确率、精确率、召回率、F1 分数）评估效果。

5. **模型部署**：
   - 将训练好的模型部署到生产环境中，进行预测或生成任务。

---

## 常见架构

### 1. **全连接网络（Fully Connected Networks, FCN）**
- 每个神经元与上一层的所有神经元相连。
- 应用场景：表格数据、基础分类任务。

### 2. **卷积神经网络（Convolutional Neural Networks, CNNs）**
- 利用卷积操作提取局部特征，适用于处理图像数据。
- 结构包括卷积层、池化层和全连接层。

### 3. **循环神经网络（Recurrent Neural Networks, RNNs）**
- 适用于序列数据（如时间序列、文本）。
- 变种包括 LSTM 和 GRU，用于解决长程依赖问题。

### 4. **Transformer**
- 基于注意力机制，能够处理长序列。
- 应用场景：自然语言处理、图像生成。

### 5. **生成对抗网络（Generative Adversarial Networks, GANs）**
- 包含生成器和判别器，用于生成数据（如图像生成、文本生成）。

---

## 深度学习的优缺点

### 优点
1. **高性能**：在大规模数据和计算资源下，深度学习模型表现出色。
2. **自动化特征提取**：无需手工设计特征。
3. **广泛应用**：适用于图像、语音、自然语言处理等多个领域。

### 缺点
1. **计算成本高**：需要强大的计算资源（如 GPU）。
2. **数据需求大**：模型训练通常需要大量标注数据。
3. **可解释性差**：模型内部的权重和结构通常很难直接解释。

---

## 应用领域

1. **计算机视觉（CV）**：
   - 图像分类、目标检测、图像分割。
2. **自然语言处理（NLP）**：
   - 机器翻译、文本生成、问答系统。
3. **语音识别**：
   - 语音转文本、语音合成。
4. **自动驾驶**：
   - 环境感知、路径规划。
5. **医疗诊断**：
   - 图像分析、疾病预测。

---

## 总结

深度学习是人工智能的重要分支，通过多层神经网络从数据中学习特征表示，能够在多种复杂任务中实现高性能。随着计算资源的增强和模型架构的优化，深度学习的应用场景不断扩大，其潜力也在逐步释放。


# Transformer 中的神经网络层构成与分类

Transformer 是一种基于注意力机制的深度学习模型，其神经网络层可以分为两大主要部分：**编码器（Encoder）** 和 **解码器（Decoder）**。每个部分由多个重复堆叠的层（Layer）组成，而每层又由特定功能的子结构模块构成。

---

## 1. **Transformer 的基础组成**

### 每层结构
Transformer 的编码器和解码器层都包含以下基本模块：

#### **1.1 多头注意力机制（Multi-Head Attention）**
- **核心功能**：通过自注意力（Self-Attention）或交互注意力（Cross-Attention）机制，使模型能够从输入序列中提取全局依赖关系。
- **组成**：
  - **输入**：查询（Query, $Q$）、键（Key, $K$）、值（Value, $V$）。
  - **运算**：基于注意力机制：
    $$
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    $$
  - **多头机制**：将注意力机制并行化：
    $$
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
    $$
    每个头有独立的参数 $W^Q, W^K, W^V$。

---

#### **1.2 前馈神经网络（Feed Forward Neural Network, FFN）**
- **核心功能**：对每个位置单独作用的两层全连接网络，用于非线性特征变换。
- **公式**：
  $$
  \text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
  $$
  - $W_1, W_2$ 是权重矩阵，$b_1, b_2$ 是偏置。

---

#### **1.3 残差连接和层归一化（Residual Connection & Layer Normalization）**
- **核心功能**：
  - **残差连接**：跳跃连接输入和输出以缓解梯度消失问题：
    $$
    \text{Output} = \text{LayerNorm}(x + \text{SubLayer}(x))
    $$
  - **层归一化**：对每层输入进行归一化，稳定训练过程。

---

#### **1.4 位置编码（Positional Encoding, PE）**
- **核心功能**：弥补 Transformer 无法捕捉序列顺序信息的缺陷。
- **公式**：
  对于位置 $pos$ 和嵌入维度 $i$：
  $$
  PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad
  PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
  $$
  **实现**：通过将位置编码与输入嵌入直接相加。

---

## 2. **分类与详细结构**

### **2.1 编码器（Encoder）层**
- **功能**：将输入序列编码为隐藏表示。
- **组成模块**：
  1. 多头自注意力机制（Self-Attention）。
  2. 前馈神经网络（FFN）。
  3. 残差连接和层归一化。

- **特点**：
  - 每个位置的表示可以关注输入序列的所有位置。
  - 结构对称，层与层之间无差别。

---

### **2.2 解码器（Decoder）层**
- **功能**：生成目标序列，每一步依赖于之前的输出和编码器的隐藏表示。
- **组成模块**：
  1. **Masked 多头自注意力**：
     - 掩码机制（Masking）确保解码器的生成只关注当前位置及之前的位置。
  2. **编码器-解码器注意力**（Encoder-Decoder Attention）：
     - 查询来自解码器输入，键和值来自编码器输出，用于对输入序列进行交互建模。
  3. 前馈神经网络（FFN）。
  4. 残差连接和层归一化。

- **特点**：
  - 解码器的自注意力是“因果性”的，确保输出的生成顺序正确。
  - 增加了与编码器交互的注意力模块。

---

## 3. **分类总结**

| **模块类别**          | **功能**                                                                                     | **公式或机制**                                                                 |
|----------------------|-------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **注意力机制**        | 捕捉序列间的全局依赖关系，分为自注意力和交互注意力。                                             | $Q, K, V$ 计算注意力分布，Softmax 归一化后加权求和值。                                |
| **前馈神经网络**      | 非线性特征变换，提高模型表达能力。                                                              | 两层线性变换：$\text{FFN}(x) = \text{max}(0, xW_1+b_1)W_2+b_2$            |
| **残差连接和归一化**  | 加速收敛，稳定训练过程，避免梯度消失。                                                          | $y = \text{LayerNorm}(x + \text{SubLayer}(x))$                              |
| **位置编码**          | 为序列数据提供位置信息。                                                                       | $\sin$ 和 $\cos$ 函数提供不同频率的信息。                                      |

---

## 4. **整体架构流程**

### 编码器：
1. 输入嵌入与位置编码相加。
2. 多头自注意力计算序列间的全局依赖。
3. 前馈神经网络进行特征变换。
4. 残差连接和归一化后输出。

### 解码器：
1. 输入嵌入与位置编码相加。
2. Masked 多头自注意力生成序列。
3. 编码器-解码器注意力引入输入序列信息。
4. 前馈神经网络特征变换。
5. 残差连接和归一化后输出。

通过上述模块的堆叠和交互，Transformer 模型能够高效捕捉序列数据中的复杂关系。

# Rouge-L
Rouge-L 是一种用于评估文本摘要质量的指标，它衡量生成摘要与参考摘要之间 n-gram 的匹配程度。其中，n-gram 指的是包含 n 个单词的连续片段。Rouge-L 通常使用最长公共子序列 (Longest Common Subsequence, LCS) 来计算 n-gram 的匹配程度



# 正则化

**正则化（Regularization）** 是在机器学习和深度学习中用于防止模型**过拟合（Overfitting）**的一种技术。它的核心思想是在损失函数中添加一个**惩罚项**，限制模型的复杂度，从而提高模型对未见数据（测试集）的泛化能力。

---

## **为什么需要正则化？**
在机器学习中，我们的目标是训练一个能很好地**泛化（generalize）**的模型，即在训练集上表现良好的同时，在测试集上也能表现良好。然而，如果模型过于复杂（比如过多的参数、过高的自由度），它可能会“记住”训练数据中的噪声，而不是学习数据的真正模式，从而导致**过拟合**。

正则化的作用就是**减少模型的复杂度，防止它过度拟合训练数据**，以提高泛化能力。

---

## **常见的正则化方法**
### **1. L1 正则化（Lasso）**
**L1 正则化** 通过在损失函数中添加参数的 **L1 范数（绝对值和）** 作为惩罚项，使某些权重变为0，从而达到特征选择的效果。

$$
L_{L1} = \lambda \sum_{i} |w_i|
$$

- **效果**：L1 正则化会让某些权重变成 **0**，因此它可以用于**特征选择**（Feature Selection）。
- **常见应用**：Lasso 回归（L1 正则化的线性回归）。
- **缺点**：当特征高度相关时，L1 可能会随机选择某些特征，而不是均匀分配权重。

---

### **2. L2 正则化（Ridge）**
**L2 正则化** 通过在损失函数中添加参数的 **L2 范数（平方和）** 作为惩罚项，使权重趋向于较小的值（接近但不等于0）。

$$
L_{L2} = \lambda \sum_{i} w_i^2
$$

- **效果**：L2 正则化会让所有权重变小，而不会完全变成 0。
- **常见应用**：Ridge 回归（L2 正则化的线性回归），深度学习中的权重衰减（Weight Decay）。
- **优点**：在特征相关性较高时，L2 比 L1 更稳定。

---

### **3. Elastic Net**
Elastic Net 结合了 L1 和 L2 正则化：

$$
L_{Elastic} = \lambda_1 \sum_{i} |w_i| + \lambda_2 \sum_{i} w_i^2
$$

- **效果**：既能稀疏化权重（L1），又能防止特征相关性问题（L2）。
- **常见应用**：当数据有很多特征，并且特征之间有较高相关性时，Elastic Net 是一个好的选择。

---

### **4. Dropout（丢弃法，常用于神经网络）**
Dropout 是深度学习中特有的正则化方法，它在每次训练时**随机“丢弃”一部分神经元**（即让它们的激活值变为0），从而降低对特定神经元的依赖，提高泛化能力。

- **实现**：训练时，每个神经元以一定概率 $ p $ 置为 0，而在测试时保持完整网络。
- **效果**：防止神经网络对某些特定路径的过度依赖，提高泛化能力。
- **应用**：在 CNN、RNN、Transformer 等深度学习模型中广泛使用。

---

### **5. Batch Normalization（批量归一化）**
虽然 Batch Normalization（BN）**主要用于加速训练**，但它也有正则化的效果：
- 通过对每个 mini-batch 进行归一化，使得数据分布更加稳定。
- 由于 BN 在训练时加入了一定的噪声（每个 mini-batch 计算的均值和方差略有不同），这起到了类似 Dropout 的正则化作用。

---

### **6. 数据增强（Data Augmentation）**
数据增强是一种间接的正则化方法，它通过增加训练样本的多样性来减少过拟合。例如：
- **图像处理**：随机旋转、翻转、裁剪、颜色变换等。
- **自然语言处理（NLP）**：同义词替换、词顺序打乱、数据扩增等。
- **语音处理**：音频加噪声、时间拉伸等。

数据增强能让模型看到更多不同的变体，提高泛化能力。

---

### **7. 早停（Early Stopping）**
在训练过程中，我们通常会监控**验证集的损失**，如果在某个点之后损失不再降低（甚至上升），说明模型开始过拟合训练集。这时我们可以**提前停止训练**，这就叫 **Early Stopping**。

- **优势**：无需额外的正则化项，只需要监控损失即可。
- **劣势**：需要一个合适的停止准则，否则可能会停止得太早或太晚。

---

## **正则化方法对比**
| 正则化方法 | 主要作用 | 适用场景 |
|-----------|---------|---------|
| **L1 正则化（Lasso）** | 让部分权重变成 0（稀疏性），用于特征选择 | 线性模型、特征稀疏化 |
| **L2 正则化（Ridge）** | 让所有权重变小，提高稳定性 | 线性模型、深度学习（权重衰减） |
| **Elastic Net** | 结合 L1 和 L2 的优势 | 适用于高度相关特征 |
| **Dropout** | 随机丢弃神经元，减少过拟合 | 深度学习（CNN、RNN、Transformer） |
| **Batch Normalization** | 归一化输入，带来一定正则化效果 | 深度学习 |
| **数据增强** | 扩展数据集，提高泛化能力 | 计算机视觉、NLP、语音处理 |
| **早停（Early Stopping）** | 监控验证集损失，避免过拟合 | 适用于所有训练过程 |

---

## **如何选择正则化方法？**
- **线性模型（回归、SVM 等）**：
  - 如果希望特征选择：**L1 正则化（Lasso）**
  - 如果希望模型稳定性：**L2 正则化（Ridge）**
  - 如果特征很多且高度相关：**Elastic Net**
  
- **深度学习（神经网络）**：
  - **Dropout**：简单有效的正则化方式，尤其适用于全连接层。
  - **Batch Normalization**：适用于大部分神经网络，提高训练稳定性，并提供一定的正则化作用。
  - **L2 正则化（Weight Decay）**：对所有参数进行约束，防止参数过大。
  - **数据增强**：计算机视觉、NLP、语音任务中不可或缺的正则化方法。
  - **Early Stopping**：最简单的正则化方法，适用于所有模型。

---

# Sinkhorn Distance 和 Wasserstein Distance

**Wasserstein Distance** 和 **Sinkhorn Distance** 都是衡量概率分布之间差异的度量，它们在计算机视觉、机器学习和计算几何等领域中都有广泛的应用。两者都来源于最优传输理论（Optimal Transport），但在计算效率和具体定义上有所不同。

---

#### 1. **Wasserstein Distance**

Wasserstein距离，又叫做**地球搬运者距离（Earth Mover's Distance, EMD）**，衡量的是两个概率分布之间的最优“运输”成本。它的定义与运输问题类似：你可以将一个分布视为需要移动的“土堆”，另一个分布视为目标地，Wasserstein距离就是将一个分布移动到另一个分布的最小成本。

对于一维离散分布，Wasserstein-1距离定义为：

$$
W_1(P, Q) = \inf_{\gamma \in \Gamma(P, Q)} \mathbb{E}_{(x, y) \sim \gamma} [\| x - y \|]
$$

其中：
- $P$ 和 $Q$ 是两个概率分布，
- $\Gamma(P, Q)$ 是所有将$P$分布映射到$Q$分布的联合分布集合（即所有可能的传输计划），
- $\mathbb{E}$ 表示期望，$\| x - y \|$是$x$和$y$之间的距离（通常取欧氏距离）。

简而言之，Wasserstein距离测量的是将一个分布通过最优传输“移动”到另一个分布所需要的成本。

#### 2. **Sinkhorn Distance**

Sinkhorn距离是Wasserstein距离的一种**正则化**形式，它在计算中引入了一个**熵正则化项**，目的是加快计算速度并且使得计算变得更加稳定。

Sinkhorn距离通过在最优传输问题的目标函数中添加熵项来逼近Wasserstein距离。这使得计算上更加高效，尤其是在高维空间或大规模数据集上。

Sinkhorn距离的公式为：

$$
S_\epsilon(P, Q) = \inf_{\gamma \in \Gamma(P, Q)} \mathbb{E}_{(x, y) \sim \gamma} [\| x - y \|] + \epsilon \cdot H(\gamma)
$$

其中：
- $\epsilon$ 是正则化参数，控制熵项的影响，$\epsilon \to 0$时，Sinkhorn距离逼近Wasserstein距离，
- $H(\gamma)$ 是联合分布$\gamma$的熵，定义为：
  
  $$
  H(\gamma) = -\sum_{x,y} \gamma(x, y) \log \gamma(x, y)
  $$

通过增加熵正则化，Sinkhorn距离变得更加平滑，避免了计算Wasserstein距离时的直接复杂度，特别适用于大规模数据和计算资源有限的场景。

---

### 对比

| **特性**            | **Wasserstein Distance**                                | **Sinkhorn Distance**                                   |
|---------------------|---------------------------------------------------------|---------------------------------------------------------|
| **定义**            | 通过最优传输衡量概率分布间的最小传输成本。              | Wasserstein距离的正则化版本，添加了熵项以加速计算。     |
| **计算复杂度**      | 计算复杂度较高，尤其是在高维空间。                     | 通过引入熵正则化，计算更加高效。                       |
| **正则化**          | 无熵正则化。                                            | 有熵正则化参数$\epsilon$，使得计算更加平滑。           |
| **应用场景**        | 用于精确计算分布间的距离，适合低维、小规模数据。      | 适合大规模数据集和高维空间，能够有效降低计算复杂度。   |

---

### 总结

- **Wasserstein Distance** 是最优传输的经典度量，能够精确地衡量两个概率分布之间的差异，适用于需要高精度距离的场景。
- **Sinkhorn Distance** 通过引入熵正则化项，提供了一个高效且稳定的计算方法，特别适合大规模数据和复杂场景。
